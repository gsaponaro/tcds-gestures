%!TEX encoding = UTF-8 Unicode

\section{Introduction}

\newcommand{\myWidthOne}{0.23}
%\newcommand{\myWidthTwo}{0.22}

\begin{figure*}
    \centering
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{grasp-00000169} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{grasp-00000170} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{grasp-00000171} } \\
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{grasp-00000173} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{grasp-00000177} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{grasp-00000180} }
    \caption{grasp}
\end{figure*}

\begin{figure*}
    \centering
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{tap-00000109} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{tap-00000110} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{tap-00000112} } \\
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{tap-00000114} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{tap-00000116} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{tap-00000117} }
    \caption{tap}
\end{figure*}

\begin{figure*}
    \centering
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{touch-00000196} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{touch-00000197} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{touch-00000198} } \\
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{touch-00000200} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{touch-00000202} } \quad
    %
    \subfloat
    { \includegraphics[width=\myWidthOne\linewidth]{touch-00000203} }
    \caption{touch}
\end{figure*}

% TODO: delete the seq2-* png files (view from external camera)
% \begin{figure*}
%     \centering
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-grasp-11} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-grasp-12} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-grasp-13} } \\
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-grasp-14} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-grasp-15} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-grasp-16} }
%     \caption{grasp}
% \end{figure*}
% 
% \begin{figure*}
%     \centering
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-tap-22} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-tap-23} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-tap-24} } \\
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-tap-25} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-tap-26} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthOne\linewidth]{seq2-tap-27} }
%     \caption{tap}
% \end{figure*}
% 
% \begin{figure*}
%     \centering
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-2} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-3} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-4} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-5} } \\
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-6} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-7} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-8} } \quad
%     %
%     \subfloat
%     { \includegraphics[width=\myWidthTwo\linewidth]{seq2-touch-9} }
%     \caption{touch}
% \end{figure*}

BELOW IS THE GLU TEXT

Robotics is progressing fast, with a steady and systematic shift from the industrial domain to domestic, public and leisure environments~\cite[ch.~65, Domestic Robotics]{siciliano:2016:handbook2}. Application areas that are particularly relevant and being researched by the scientific community include: robots for people's health and active aging, mobility, advanced manufacturing~(Industry~4.0). In short, all domains that require direct and effective \hri{} and communication (including language and gestures~\cite{matuszek:2014:aaai}).

However, robots have not reached the level of performance that would enable them to work with humans in routine activities in a flexible and adaptive way, for example in the presence of sensor noise, or unexpected events not previously seen during the training or learning phase. One of the reasons to explain this performance gap between \hh{} teamwork and a \hr{} teamwork is in the collaboration aspect, i.e., whether the members of a team understand one another. Humans have the ability of working successfully in groups. They can agree on common goals~(e.g., through verbal and non-verbal communication), work towards the execution of these goals in a coordinated way, and understand each other's physical actions~(e.g., body gestures) towards the realization of the final target. Human team coordination and mutual understanding is effective~\cite{ramnani:2004:natureneuro} because of~(i) the capacity to \emph{adapt} to unforeseen events in the environment, and re-plan one's actions in real time if necessary, and~(ii) a common motor repertoire and action model, which permits us to understand a partner's physical actions and manifested intentions as if they were our own~\cite{saponaro:2013:crhri}.

In neuroscience research, visuomotor neurons~(i.e., neurons that are activated by visual stimuli) have been a subject of ample study~\cite{rizzolatti:2001:nrn}. Mirror neurons are one class of such neurons that responds to action and object interaction, both when the agent acts and when it observes the same action performed by others, hence the name ``mirror''.

\begin{figure}
  \centering
  \includegraphics[width=0.9\columnwidth]{human_tap}
  \caption{Experimental setup, consisting of an iCub humanoid robot and a human user performing a manipulation gesture on a shared table with different objects on top. The depth sensor in the top-left corner is used to extract human hand coordinates for gesture recognition. Depending on the gesture and on the target object, the resulting effect will differ.}
  \label{fig:experimental_setup}
\end{figure}

This work takes inspiration from the theory of mirror neurons, and contributes towards using it on humanoid and cognitive robots. We show that a robot can first acquire knowledge by sensing and self-exploring its surrounding environment~(e.g., by interacting with available objects and building up an affordance representation of the interactions and their outcomes) and, as a result, the robot is capable of generalizing its acquired knowledge while observing another agent~(e.g., a human person) who performs similar physical actions to the ones executed during prior robot training. Fig.~\ref{fig:experimental_setup} shows the experimental setup.
