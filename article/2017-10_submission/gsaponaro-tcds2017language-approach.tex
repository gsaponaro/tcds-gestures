%!TEX encoding = UTF-8 Unicode
\section{Proposed Approach}
The purpose of the proposed approach is to reuse the experience that the robot acquires when playing alone to interpret other people's actions.
In order to achieve this, we combine (1)~the robot affordance model of~\cite{salvi:2012:smcb}, which associates verbal descriptions to the physical interactions of an agent with the environment, with (2)~the gesture recognition system of~\cite{saponaro:2013:crhri}, which infers the type of action from human user movements.
We consider three \emph{manipulative gestures} corresponding to physical actions performed by agent(s) onto objects on a table~(see Fig.~\ref{fig:experimental_setup}): grasp, tap, and touch.
We reason on the effects of these actions onto the objects of the world, and on the co-occurring verbal description of the experiments.

A possible representation of the probabilistic dependencies that the robot needs to acquire in order to associate other people's actions to verbal descriptions is depicted in Figure~\ref{fig:model}~(a).
The intended actions result in movements of the body that can be measured in terms of \emph{gesture features}.
These movements, in turns, may determine some \emph{effects}, on the surroundings expressed by a number of relevant measurable variables.
Those effects are also determined by the particular \emph{object features} associated with the objects that are involved in the action and that are also observable.
Finally, an observer composes a verbal description of the situation based on all the observable variables.

By combining the affordance model in \cite{salvi:2012:smcb} and the gesture recognition system in \cite{saponaro:2013:crhri}, however, we need to do some simplifying assumptions, that are depicted in Figure~\ref{fig:model}~(b).
The main difference, is that, in the learning phase in \cite{salvi:2012:smcb}, the intended action was known to the robot.
The robot could, therefore, learn direct dependencies between the actions and the other variables in the affordance-word model depicted in the corresponding dashed box in Figure~\ref{fig:model}~(b).
Although some of the effect variables refer to the robot's arm movements, this information was not rich enough to recognize each particular gesture and could therefore not be used directly to interpret other people's behavior.

This limitation can be partly overcome by the gesture/action recognition model of~\cite{saponaro:2013:crhri} which can recover the particular intended action from the gesture features.
Consequently, the relationship between gesture features, object features, effects and words, that we described earlier, are mediated by the actions variable in our model.
The combination of the two models is our main contribution and allows us to relax the assumption made in \cite{salvi:2012:smcb} that the action is known during the learning phase.

In the following, we give details about the affordance-word model and the gesture/action recognition model employed in the study and on the way we do inference by merging the information provided by those two models.

%BELOW IS THE GLU TEXT

%In this paper, we combine (1)~the robot affordance model of~\cite{salvi:2012:smcb}, which associates verbal descriptions to the physical interactions of an agent with the environment, with (2)~the gesture recognition system of~\cite{saponaro:2013:crhri}, which infers the type of action from human user movements.
%We consider three \emph{manipulative gestures} corresponding to physical actions performed by agent(s) onto objects on a table~(see Fig.~\ref{fig:experimental_setup}): grasp, tap, and touch.
%We reason on the effects of these actions onto the objects of the world, and on the co-occurring verbal description of the experiments. In the complete framework, we will use \acfp{BN}, which are a probabilistic model that represents random variables and conditional dependencies on a graph, such as in Fig.~\ref{fig:model}. One of the advantages of using \acp{BN} is that their expressive power allows the marginalization over any set of variables given any other set of variables.

%Our main contribution is that of extending~\cite{salvi:2012:smcb} by relaxing the assumption that the action is known during the learning phase.
%This assumption is acceptable when the robot learns through self-exploration and interaction with the environment, but must be relaxed if the robot needs to generalize the acquired knowledge through the observation of another~(human) agent.
%We estimate the action performed by a human user during a \hr{} collaborative task, by employing statistical inference methods and \acp{HMM}. This provides two advantages. First, we can infer the executed action during training. Secondly, at testing time we can merge the action information obtained from gesture recognition with the information about affordances.


\subsection{\AffWords{} Modeling}
\label{sec:bn}

Following the method adopted in~\cite{salvi:2012:smcb}, we use a Bayesian probabilistic framework to allow a robot to ground the basic world behavior and verbal descriptions associated to it.
All variables in the model are discrete or are discretized from continuous sensory variables through clustering in a preliminary learning phase.
Details on the variable definition and possible values are given in Table~\ref{tab:bnsymb}.

We call $X = \{a, f_1, f_2, \dots, e_1, e_2, \dots\}$ the combination of all the affordance variables that corresponds to the state of the world as experienced by the robot. The verbal descriptions are denoted by the set of words~$W = \{w_i\}$. Consequently, the relationships between words and concepts are expressed by the joint probability distribution~$p(X,W)$ of actions, object features, effects, and words in the spoken utterance.

\begin{table}
    \centering
    \caption{The symbolic variables of the \acl{BN} (from~\cite{salvi:2012:smcb}), with the corresponding discrete values obtained from clustering during robot exploration of the environment.}
    \label{tab:bnsymb}
    \begin{tabular}{clp{2.7cm}p{2.5cm}}
    \toprule
    symbol & name   & description     & values \\
    \midrule
    $a$ & Action & action          & grasp, tap, touch \\
    \midrule
    $f_1$ & Color   & object color   & blue, yellow, green1, green2 \\
    $f_2$ & Size   & object size     & small, medium, big \\
    $f_3$ & Shape  & object shape    & sphere, box \\
    \midrule
    $e_1$ & ObjVel & object velocity & slow, medium, fast \\
    $e_2$ & HandVel & robot hand velocity & slow, fast \\
    $e_3$ & ObjHandVel & relative object hand velocity & slow, medium, fast \\
    $e_4$ & Contact & object hand contact & short, long \\
    \midrule
    $w_i$ & word$_i$ & presence of word$_i$ in the verbal description & binary \\
    \bottomrule
    \end{tabular}
\end{table}

This joint probability distribution is modeled by a Bayesian network sketched in the part of Fig.~\ref{fig:model}~(b) enclosed in the Affordance-word model dashed box.
The dependency structure and the model parameters are estimated by the robot in an ego-centric way through interaction with the environment, as in~\cite{salvi:2012:smcb}.
As a consequence, during learning, the robot knows what action it is performing with certainty, and the variable~$A$ assumes a deterministic value. This assumption is relaxed in the present study, by extending the model to the observation of external~(human) agents as explained below.

\subsection{Gesture/Action Recognition}

\newcommand{\myscalefactor}{0.8}

\newcommand{\standardhmm}[1]{
    \node[draw,circle] (hmm#1s1) {$s_1$};
    \node[draw,circle, right of=hmm#1s1] (hmm#1s2) {$s_2$};
    \node[circle, right of=hmm#1s2] (hmm#1s3) {\dots};
    \node[draw,circle, right of=hmm#1s3] (hmm#1s4) {$s_Q$};
    \node[left of=hmm#1s1]  (invisible1) {};
    \node[right of=hmm#1s4] (invisible2) {};
    \path[->] (hmm#1s1) edge (hmm#1s2);
    \path[loop above] (hmm#1s1) edge (hmm#1s1);
    \path[->] (hmm#1s2) edge (hmm#1s3);
    \path[loop above] (hmm#1s2) edge (hmm#1s2);
    \path[dashed] (hmm#1s2) -- (hmm#1s3);
    \path[->] (hmm#1s3) edge (hmm#1s4);
    \path[loop above] (hmm#1s4) edge (hmm#1s4);
    %\path[->] (hmm#1s4) edge[bend left] (hmm#1s1);
    \path[->] (invisible1) edge (hmm#1s1);
    \path[->] (hmm#1s4) edge (invisible2);
}

\newcommand{\modeltwo}{
  \begin{tikzpicture}[scale=\myscalefactor, every node/.style={scale=\myscalefactor}]
  \matrix (M) [matrix of nodes, ampersand replacement=\&] {%
    grasp gesture HMM \& \standardhmm{1} \\
    tap gesture HMM \& \standardhmm{2} \\
    touch gesture HMM \& \standardhmm{3} \\
    %garbage \& \standardhmm{4} \\
  };
  \end{tikzpicture}
}

\begin{figure}
  \centering
  \modeltwo
  \caption{Structure of the \acp{HMM} used for human gesture recognition, adapted from~\cite{saponaro:2013:crhri}. In this work, we consider three independent, multiple-state \acp{HMM}, each of them trained to recognize one of the considered manipulation gestures.}
  \label{fig:hmms}
\end{figure}

The gesture/action recognition system is based on hidden Markov models with Gaussian emitting probability distributions as in~\cite{saponaro:2013:crhri}.
The gesture recognition models are represented in Fig.~\ref{fig:hmms}, and correspond to the Gesture/Action recognition block in Fig.~\ref{fig:model}~(b).

%As for the gesture recognition \acsp{HMM}, we use the models that we previously trained in~\cite{saponaro:2013:crhri} for spotting the manipulation-related gestures under consideration.
Our input features are the 3D coordinates of the tracked human hand indicated by the $g_i$ variables in Figure~\ref{fig:model}.
The coordinates are obtained with a commodity depth sensor, then transformed to be centered on the person torso~(to be invariant to the distance of the user from the sensor) and normalized to account for variability in amplitude~(to be invariant to wide/emphatic vs narrow/subtle executions of the same gesture class).

The model for each gesture/action is a so called left-to-right \ac{HMM} where the transition model between the discrete states $\mathcal{S} = \{s_1, \dots, s_Q\}$ is structured so that states with lower index represent events that occur earlier in time.
%set of (hidden) discrete states~$\mathcal{S} = \{s_1, \dots, s_Q\}$ which model the temporal phases comprising the dynamic execution of the gesture, and by a set of parameters~$\lambda = \{ A, B, \Pi \}$, where~$A = \{ a_{ij} \}$ is the transition probability matrix, $a_{ij}$ is the transition probability from state~$s_i$ at time~$t$ to state~$s_j$ at time~$t+1$, $B = \{ f_i \}$ is the set of $Q$~observation probability functions~(one per state~$i$) with continuous mixtures of Gaussian values, and~$\Pi$ is the initial probability distribution for the states.

Although not expressed so far in the notation, the continuous variables $g_i$ are measured at regular time intervals.
At a certain time step $n$ the feature vector can be expressed as $\mathbf{g}[n] = \{g_1[n], \dots g_D[n]\}$.
The input to the model is a sequence of $N$ such feature vectors $\mathbf{g}[1], \dots, \mathbf{g}[N]$ that we call for simplicity $G_1^N$, where $N$ can vary for every recording.

At recognition~(testing) time, we can use the models to estimate the likelihood of a new sequence of observations $G_1^N$, given each possible gesture/action by means of the \FB{} inference algorithm.
We can express this likelihood as $\mathcal{L}_\text{HMM}(G_1^N|a)$, where $a$ is one of the possible actions.
By normalizing the likelihoods, assuming that the gestures are equally likely \emph{a priori}, we can obtain the posterior probability of the action given the sequence of observations that we call $\phmm(a|G_1^N)$.

%In Sec.~\ref{sec:combination}, we discuss different ways in which the output information of the gesture recognizer can be combined with the \acl{BN} of words and affordances.

\newcommand{\pcomb}{\ensuremath P_{\text{COMB}}}

\subsection{Combining the \acs{BN} with Gesture \acsp{HMM}}
\label{sec:combination}
The two models described above define two probability distributions over the relevant variables for the problem:
\begin{eqnarray*}
  \pbn(a, F, E, W) &=& \pbn(a, f_1, f_2, \dots, e_1, e_2, \dots,\\
  && \hspace{1cm} \dots, w_1, w_2, \dots), \text{ and} \\
  \phmm(a|G_1^N) &&
\end{eqnarray*}
The goal is to merge the information provided by both models and estimate $\pcomb(a, F, E, W|G_1^N)$, that is, the joint probability of all the affordance and word variables, given that we observe a certain gesture/action performed by the human.

During inference, we will have a (possibly empty) set of observed variables $X_\text{OBS} \subseteq (a \cup F \cup E \cup W)$, and a set of variables on which we wish to perform the inference: $X_\text{INF} \subseteq (a \cup F \cup E \cup W)$.
In order for the inference to be non-trivial it must be $X_\text{OBS} \cap X_\text{INF} = \varnothing$, that is, we should not observe any inference variable.
According to the Bayesian network alone, the inference will compute the probability distribution of the inference variables $X_\text{INF}$ given the observed variables $X_\text{OBS}$ by marginalizing over all the other (latent) variables $X_\text{LAT} = (a \cup F \cup E \cup W) \setminus (X_\text{OBS} \cup X_\text{INF})$, where $\setminus$ is the set difference operation:
\begin{eqnarray*}
 \pbn(X_\text{INF}|X_\text{OBS}) = \sum_{X_\text{LAT}} \pbn(X_\text{INF}, X_\text{LAT}| X_\text{OBS})
\end{eqnarray*}

If we want to combine the evidence brought by the Bayesian network with the evidence brought by the hidden Markov model, there are two cases that can occur:
\begin{enumerate}
\item the variable action is included among the inference variables: $a \in X_\text{INF}$
\item the variable action is not included among the inference variables: $a \in X_\text{LAT}$.
\end{enumerate}
We are here excluding the case that we observe the action directly ($a\in X_\text{OBS}$) for two reasons: firstly, this would correspond to the robot perfroming the action himself, whereas we are interested in interpreting other people's actions;
secondly, this would make the evidence on the gesture features $G_1^N$ irrelevant, because in the model in Figure~\ref{fig:model}~(b), there is a tail-to-tail connection from $G_1^N$ to the rest of the variables through the action variable, which means that, given the action, all dependencies to the gesture features are dropped. 
The two cases enumerated above can be addressed separately when we do inference.

In the first case, we call $X_\text{INF}^\prime$ the set of inference variables excluding the action $a$, that is, $X_\text{INF} = \{X_\text{INF}^\prime, a\}$.
We can write:
\begin{align*}
  & \pcomb(X_\text{INF}| X_\text{OBS}, G_1^N) = \pcomb(a, X_\text{INF}^\prime| X_\text{OBS}, G_1^N)= \\
  &= \pbn(a, X_\text{INF}^\prime| X_\text{OBS}, G_1^N) \phmm(a, X_\text{INF}^\prime| X_\text{OBS}, G_1^N)= \\
  &= \pbn(a, X_\text{INF}^\prime| X_\text{OBS}) \phmm(a, | G_1^N) \\
  &= \pbn(X_\text{INF}| X_\text{OBS}) \phmm(a, | G_1^N).
\end{align*}

EXPLAIN WHAT THIS MEANS

In the case where the action is among the latent variables, 

GLU TEXT

In this study we wish to generalize the model of~\cite{salvi:2012:smcb} by observing external~(human) agents, as shown in Fig.~\ref{fig:experimental_setup}. For this reason, the full model is now extended with a perception module capable of inferring the action of the agent from visual inputs. This corresponds to the Gesture \acp{HMM} block in Fig.~\ref{fig:model}. The \AffWords{} \acf{BN} model and the Gestures \acp{HMM} may be combined in different ways~\cite{pan:2006:ictai}:
\begin{enumerate}
\item the Gesture \acp{HMM} may provide a hard decision on the action performed by the human~(i.e., considering only the top result) to the \ac{BN},

\item the Gesture \acp{HMM} may provide a posterior distribution~(i.e., soft decision) to the \ac{BN},

\item if the task is to infer the action, the posterior from the Gesture \acp{HMM} and the one from the \ac{BN} may be combined as follows, assuming that they provide independent information:
\begin{equation*}
p(A) = \phmm(A) \, \pbn(A).
\end{equation*}
\end{enumerate}

In the experimental section, we will show that what the robot has learned subjectively or alone~(by self-exploration, knowing the action identity as a prior~\cite{salvi:2012:smcb}), can subsequently be used when observing a new agent~(human), provided that the actions can be estimated with Gesture \acp{HMM} as in~\cite{saponaro:2013:crhri}.
