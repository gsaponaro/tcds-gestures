%!TEX encoding = UTF-8 Unicode

\section{Experimental Settings}
\label{sec:experimental_settings}
Our experiments consist on testing our method on a number of example scenarios that will be described in Sec.~\ref{sec:results}.
In this section we provide experimental details and key assumptions in the method.

\begin{table}
    \centering
    \caption{The symbolic variables of the \acl{BN} (from~\cite{salvi:2012:smcb}), with the corresponding discrete values obtained from clustering during robot exploration of the environment.}
    \label{tab:bnsymb}
    \begin{tabular}{cp{3.3cm}l}
    \toprule
    symbol & name: description     & values \\
    \midrule
    $a$    & Action: motor action  & grasp, tap, touch \\
    \midrule
    $f_1$  & Color: object color   & blue, yellow, green1, green2 \\
    $f_2$  & Size: object size     & small, medium, big \\
    $f_3$  & Shape: object shape   & sphere, box \\ % in the implementation the value is called circle, but sphere is clearer
    \midrule
    $e_1$  & ObjVel: object velocity                     & slow, medium, fast \\
    $e_2$  & HandVel: robot hand velocity                & slow, fast \\
    $e_3$  & ObjHandVel: relative \objecthand{} velocity & slow, medium, fast \\
    $e_4$  & Contact: object hand contact                & short, long \\
    \midrule
    $w_1$--$w_{49}$ & presence of each word in the verbal description & true, false \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{\AffWords{} Model}
Table~\ref{tab:bnsymb} presents a list of variables and the corresponding values used in the \AffWords{} model.
Note that the name of the values have been assigned by us arbitrarily to the clusters, for the sake of making the results more human-interpretable.
However, the robot has no prior knowledge about the meaning of these clusters nor their order in case they correspond to ordered quantities.
In order to extract object features and effects from the sensory data we assume that the robot possesses visual segmentation and geometric reasoning capabilities, meaning that it is able to segment the~(potentially multiple) regions of interest corresponding to the physical objects of the world from the background~(e.g., a planar surface such as a table) and to determine their positions.

We use the following notation in order to distinguish between the values of the affordance variables~(all but the last row in Table~\ref{tab:bnsymb}) and the words~(last row in the table).
Words and sentences are always enclosed in quotation marks.
For example, ``sphere'' will refer to the spoken word, whereas sphere will refer to the value of the Shape variable corresponding to the specific cluster.
Similarly, ``grasp'' will correspond to a spoken word, whereas grasp correponds to a value of the Action variable.
There is no one-to-one correspondence between the values of the affordance variables and words.
Situations are described by humans with variability:
(i)~there are many synonyms for the same concept: for instance, cubic object are called ``box'', ``square'' or ``cube'';
(ii)~different affordance variable values may have the same associated verbal description, e.g., two color clusters corresponding to different shades of green are both referred to as ``green'';
(iii)~finally, many affordance variable values have no direct description: for example, the object velocity and \objecthand{} velocity~(slow, medium, fast), or the \objecthand{} contact~(short, long) are never described directly, and need to be inferred from the situation.

\subsection{Gesture Recognition}
\label{sec:experimental_settings:gesture_recognition}
In this work, we consider three independent, multiple-state \aclp{HMM}, each of them trained to recognize one of the considered manipulation gestures of  Fig.~\ref{fig:action_examples}.
The 3D coordinates of the human limbs and torso used to extract the input to the gesture recognizer are obtained with a commodity depth sensor~(Kinect).

Currently, our gesture recognition algorithm relies on human skeleton tracking software from a depth stream.
In our experience, the hand tracking is not reliable in the presence of a tabletop~(i.e., partially occluded human) as in Fig.~\ref{fig:action_examples}, so we record the same gestures twice, with and without the table: the latter is used for ensuring the robustness of the estimated hand coordinate, the former is used throughout the rest of our model and experiments.
We plan to overcome this limitation in future work.
