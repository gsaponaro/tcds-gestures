%!TEX encoding = UTF-8 Unicode

\section{Conclusions and Future Work}
\label{sec:conclusions}

We have presented a model that combines robot ego-centric learning about language and object affordances with the observation of external agents' gestures.
We have illustrated a novel probabilistic method to fuse these two sources of information, and we have shown a number of experiments where the robotic model reasons over affordances and words when observing another agent.
Interestingly, these predictions of words allow us to create human-interpretable sentences~(from a pre-defined grammar) which highlight the emergence of language properties in the model, such as:
congruent/incongruent conjunctions,
choice of appropriate synonym words. % when referring to the same entity in two consecutive sentences.
We make our human action data and probabilistic reasoning code publicly available.

While our scenario and data are restricted to a particular scenario~(a human and a robot manipulating simple objects on a shared table, a limited number of motor actions and effects, and a vocabulary of approximately~$50$ words to describe the experiments verbally), we believe that our study has merit because it spans different fields such as robot learning, language grounding, and object affordances.

%\subsection{Assumptions and Future Work}
The model presented in this study was created following a number of assumptions, which can be relaxed in future work (TODO turn into text, discuss how much to say here):
% Giampiero: I write this here in items, but it should be turned into text
% we should also discuss how much to say explicitely
\begin{itemize}
\item the gesture recognition part relies on depth sensor hardware~(e.g., Kinect) and human skeleton tracking software. In our experience, the hand tracking is not reliable in the presence of a tabletop~(i.e., partially occluded human) as in Fig.~\ref{fig:action_examples}, so we had to record the same gestures without the table. We plan to incorporate a robust color camera-based human joint estimator such as OpenPose~\cite{cao:2017:openpose-cpvr};
\item the model does not learn the grammar to generate verbal descriptions from word probabilities, but it is specified manually;
\item the model does not permit to add new words to its vocabulary (refer to TODO)
\item except for the gesture/action recognizer, the rest of our model has no time evolution $\rightarrow$ try dynamic Bayesian networks?
\item multiple object or multiple interlocutors are not possible;
\item results were qualitative, rather than having a numerical evaluation.
\end{itemize}

% Giampiero: not that relevant any longer
%PERSONAL NOTES - Things we need to discuss:
%\begin{itemize}
%\item the effect variables in the BN include hand velocity. How does this relate to the gesture features in the HMM? Can we learn the connection?
%\item in order to achieve the ideal case in Figure~\ref{fig:model} we would need to learn the complex relationship between hand movements and affordance variables:
%  \begin{itemize}
%  \item how can this be achieved in the ego-centric learning? Mixture of HMMs and BN? Would it be possible to learn the dependency structure in this complex model?
%  \item assuming it is possible to learn in the ego-centric way, how can this be extended to the other agent? Coordinates will definitely be different. In the current study we could directly map the two because we were using the categorical variable ``action''.
%  \end{itemize}
%\end{itemize}

% BELOW IS THE GLU TEXT
%
% Within the scope of cognitive robots that operate in unstructured environments, we have discussed a model that combines word affordance learning with body gesture recognition. We have proposed such an approach, based on the intuition that a robot can generalize its previously-acquired knowledge of the world~(objects, actions, effects, verbal descriptions) to the cases when it observes a human agent performing familiar actions in a shared \hr{} environment. We have shown promising preliminary results that indicate that a robot's ability to predict the future can benefit from incorporate the knowledge of a partner's action, facilitating scene interpretation and, as a result, teamwork.
%
% In terms of future work, there are several avenues to explore. The main ones are (i)~the implementation of a fully probabilistic fusion between the affordance and the gesture components~(e.g., the soft decision discussed in Sec.~\ref{sec:combination}); (ii)~to run quantitative tests on larger corpora of \hr{} data; (iii)~to explicitly address the correspondence problem of actions between two agents operating on the same world objects~(e.g., a pulling action from the perspective of the human corresponds to a pushing action from the perspective of the robot, generating specular effects).
