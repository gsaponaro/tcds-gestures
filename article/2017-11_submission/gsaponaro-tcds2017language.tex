%!TEX encoding = UTF-8 Unicode
\pdfoutput=1 % tell arXiv to use PDFLaTeX

%\documentclass[peerreview,draftcls,onecolumn,letterpaper,twoside]{IEEEtran} % Giampiero 2012
\documentclass[journal]{IEEEtran}

\usepackage[T1]{fontenc} % output - specifies encoding used in fonts; needs full LaTeX distribution to produce good-looking output
\usepackage[utf8]{inputenc} % input - type accented characters directly from keyboard
\usepackage[english]{babel} % internationalization - hyphenation, typographic rules for one or more languages

\usepackage[nounderscore]{syntax} % definition of the context-free grammar

\usepackage[
  backend=biber, % bibliography engine
  style=ieee, % bibliography style .bbx and citation style .cbx
  hyperref, % make citations and references clickable - requires hyperref pkg
  maxbibnames=99, % 99 = display all authors of multi-author articles
  doi=false, % do not display DOI
  isbn=false, % do not display ISBN
]{biblatex} % note: incompatible with ucs (-> utf8x), natbib
\renewcommand{\bibfont}{\footnotesize}
\usepackage[
  autostyle % adapt citation style to current document language
]{csquotes} % Context Sensitive Quotations; provides biblatex \enquote{}

\usepackage[nolist]{acronym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm} % bold math
\usepackage{booktabs}
\usepackage{graphicx}
\newcommand{\myWidth}{0.16} % used in various figures involving photos
\usepackage[multiple]{footmisc} % multiple footnote at the same point
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{url}

\usepackage{tikz}
\usetikzlibrary{matrix,positioning}
\usetikzlibrary{decorations.text}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{fit}
\tikzstyle{circle}=[shape=circle,minimum size=0.7cm,very thick]
\tikzstyle{every path}=[very thick]
\usetikzlibrary{arrows}

%\usepackage{hyperref}
\usepackage[update,prepend]{epstopdf}
\graphicspath{{figures/}{bios/}}

\addbibresource{lang_journal_bibliography.bib}

% custom commands and frequent expressions that require typesetting care
\newcommand{\actioneffect}{action--effect}
\newcommand{\AffWords}{Affordance--Words}
%\newcommand{\affwords}{affordance--words}
\newcommand{\FB}{Forward--Backward}
\newcommand{\HR}{Human--Robot}
\newcommand{\HRI}{\HR{} Interaction}
\newcommand{\hh}{human--human}
\newcommand{\hr}{human--robot}
\newcommand{\hri}{\hr{} interaction}
\newcommand{\ObjAct}{Object--Action}
\newcommand{\objecthand}{object--hand}
\newcommand{\SensMot}{Sensory--Motor}

\newcommand{\phmm}{\ensuremath{P_{\text{HMM}}}}
\newcommand{\pbn}{\ensuremath{P_{\text{BN}}}}
\newcommand{\pcomb}{\ensuremath{P_\text{comb}}}
\newcommand{\xinf}{\ensuremath{X_\text{inf}}}
\newcommand{\xobs}{\ensuremath{X_\text{obs}}}
\newcommand{\xlat}{\ensuremath{X_\text{lat}}}

%\newcommand{\given}{\ensuremath{|}} % Giampiero, without space around bar
\newcommand{\given}{\ensuremath{\mid}} % Giovanni, with space around bar

% list of acronyms
\begin{acronym}
\acro{AI}{Artificial Intelligence}
\acro{BN}{Bayesian Network}
\acro{CFG}{context-free grammar}
\acro{HMM}{Hidden Markov Model}
\acro{MTRNN}{Multiple Timescales Recurrent Neural Network}
\acro{OAC}{\ObjAct{} Complex}
\acrodefplural{OAC}{\ObjAct{} Complexes}
\acro{PDF}{Probability Density Function}
\end{acronym}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Beyond the Self: Using Grounded Affordances to Interpret and Describe Others' Actions}

\author{Giovanni~Saponaro,~\IEEEmembership{Student Member,~IEEE,}
        Lorenzo~Jamone,~\IEEEmembership{Member,~IEEE,}
        Alexandre~Bernardino,~\IEEEmembership{Member,~IEEE}
        Giampiero~Salvi,~\IEEEmembership{Member,~IEEE}
        \thanks{
          Manuscript received November 15, 2017; revised xxx mm, yyyy. This research was supported by the FCT project~UID/EEA/50009/2013 and by the CHIST-ERA project IGLU.}
\thanks{G.~Saponaro and A.~Bernardino are with the
Institute for Systems and Robotics, Instituto Superior Técnico,
Universidade de Lisboa, Lisbon, Portugal, e-mail: \{gsaponaro,alex\}@isr.tecnico.ulisboa.pt.}
\thanks{L.~Jamone is with ARQ~(Advanced Robotics at Queen Mary), School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom
and with the
Institute for Systems and Robotics, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal,
e-mail: l.jamone@qmul.ac.uk.}
\thanks{G.~Salvi is with KTH Royal Institute of Technology, Stockholm, Sweden,
  e-mail: giampi@kth.se.}
}

% make the title area
\maketitle
\IEEEpeerreviewmaketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  \input{abstract}
%With time and with social experience, children develop several fundamental skills, such as
%(i)~understanding the physical actions of caregivers and other people, for example when collaborating to achieve a shared goal or game, and
%(ii)~narrative abilities, i.e., not only learning language, but learning to use it for describing physical events, using the adequate grammar, producing the correct utterances.
%However, when it comes to robots, they still have a way to go in that regard, because of the difficulty of modeling and interpreting the real world, other agents, and the subtleties inherent to language and communication: all of these factors contribute to making the world highly variable, unstructured, and unpredictable.
%In this paper, we hypothesize that a humanoid robot can generalize its previously-acquired knowledge of the world~(objects, physical actions, effects, verbal descriptions) to the cases when it observes a human agent performing familiar actions in a shared \hr{} environment.
%We propose a probabilistic method to fuse self-learned knowledge with the observation of other human agents.
%We report results about how our model is able to do predictions and inferences from these conjoint sources of information, as well as generating verbal descriptions of \hr{} collaboration scenarios with manipulative actions, and acquiring non-trivial language concepts.
\end{abstract}

\begin{IEEEkeywords}
affordances, embodied cognition, gestures, humanoid robots, language acquisition through development.
\end{IEEEkeywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{related_work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{method}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{experimental_settings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{conclusions}

\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\printbibliography

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\input{bios}

\end{document}
