%!TEX encoding = UTF-8 Unicode

\section{Experimental Settings}
\label{sec:experimental_settings}
Our experiments consist on testing our method on a number of example scenarios that will be described in Sec.~\ref{sec:results}.
In this section we provide experimental details and key assumptions in the method.

\begin{table}
    \centering
    \caption{The symbolic variables of the \acl{BN} (from~\cite{salvi:2012:smcb}), with the corresponding discrete values obtained from clustering during robot exploration of the environment.
    We call \emph{word variables} the booleans of the last row, whereas we call \emph{affordance variables} all the other symbols.}
    \label{tab:bnsymb}
    \begin{tabular}{cp{3.3cm}l}
    \toprule
    symbol & name: description     & values \\
    \midrule
    $a$    & Action: motor action  & grasp, tap, touch \\
    \midrule
    $f_1$  & Color: object color   & blue, yellow, green1, green2 \\
    $f_2$  & Size: object size     & small, medium, big \\
    $f_3$  & Shape: object shape   & sphere, box \\ % in the implementation the value is called circle, but sphere is clearer
    \midrule
    $e_1$  & ObjVel: object velocity                     & slow, medium, fast \\
    $e_2$  & HandVel: robot hand velocity                & slow, fast \\
    $e_3$  & ObjHandVel: relative \objecthand{} velocity & slow, medium, fast \\
    $e_4$  & Contact: object hand contact                & short, long \\
    \midrule
    $w_1$--$w_{49}$ & presence of each word in the verbal description & true, false \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{\AffWords{} Model}
Table~\ref{tab:bnsymb} presents a list of variables and the corresponding values used in the \AffWords{} model.
Note that the name of the values of the affordance variables have been assigned by us arbitrarily to the clusters, for the sake of making the results more human-interpretable.
However, the robot has no prior knowledge about the meaning of these clusters nor their order in case they correspond to ordered quantities.
In order to extract object features and effects from the sensory data we assume that the robot possesses visual segmentation and geometric reasoning capabilities, meaning that it is able to segment the~(potentially multiple) regions of interest corresponding to the physical objects of the world from the background~(e.g., a planar surface such as a table) and to determine their positions.

We use the following notation in order to distinguish between the values of the affordance variables~(all but the last row in Table~\ref{tab:bnsymb}) and the words~(last row in the table).
Words and sentences are always enclosed in quotation marks.
For example, ``sphere'' will refer to the spoken word, whereas sphere will refer to the value of the Shape variable corresponding to the specific cluster.
Similarly, ``grasp'' will correspond to a spoken word, whereas grasp corresponds to a value of the Action variable.

There is no one-to-one correspondence between the values of the affordance variables and words.
This was partly emerging from the natural variability that is inherent in the way humans describe situations in spoken words.
It was also a design choice, because we wanted to prove that the model was not merely able to recover simple word--meaning associations, but was able to cope with more natural spoken utterances.
Consequently, in the spoken descriptions:
(i)~there are many synonyms for the same concept: for instance, cubic object are called ``box'', ``square'' or ``cube''. Also, actions and effects are described using different tenses (``is grasping'', ``grasped'', ``has (just) grasped'');
(ii)~different affordance variable values may have the same associated verbal description, e.g., two color clusters corresponding to different shades of green are both referred to as ``green'';
(iii)~finally, many affordance variable values have no direct description: for example, the object velocity and \objecthand{} velocity~(slow, medium, fast), or the \objecthand{} contact~(short, long) are never described directly, and need to be inferred from the situation.

The \AffWords{} model does not account for the concepts of parts of speech, verb tenses or \emph{temporal aspects} explicitly.
For example, the words ``is'', ``grasping'', ``has'', ``grasped'', ``just'', and so on, are initially completely distinct and unrelated to the model, which has no prior information about what verbs, adjectives or nouns are, nor about similarity between words.
It is only through the association with the other robot observations that the model realizes that ``grasping'' has the same meaning as ``grasped''.
The following three phrases, which were used interchangeably in the experiments, are mapped to exactly the same meaning, after learning:
(i)~``is grasping'',
(ii)~``has grasped'',
(iii)~``grasped''.
Note that the model \emph{per~se} would be fully capable to distinguish between those phrases, provided that they were used in different situations, which however was not the case in our experimental data.

\subsection{Gesture Recognition}
\label{sec:experimental_settings:gesture_recognition}
In this work, we consider three independent, multiple-state \aclp{HMM}, each of them trained to recognize one of the considered manipulation gestures of  Fig.~\ref{fig:action_examples}.
The 3D coordinates of the human limbs and torso used to extract the input to the gesture recognizer are obtained with a commodity depth sensor~(Kinect)\footnote{Currently, our gesture recognition algorithm relies on human skeleton tracking software from a depth stream.
In our experience, the hand tracking is not reliable in the presence of a tabletop~(i.e., partially occluded human) as in Fig.~\ref{fig:action_examples}, so we record the same gestures twice, with and without the table: the latter is used for ensuring the robustness of the estimated hand coordinate, the former is used throughout the rest of our model and experiments.
We plan to overcome this limitation in future work.}.
